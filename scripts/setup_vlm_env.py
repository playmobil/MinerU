#!/usr/bin/env python3
"""
MinerU VLM ç¯å¢ƒè®¾ç½®è„šæœ¬
è‡ªåŠ¨æ£€æµ‹ç¡¬ä»¶ç¯å¢ƒå¹¶è®¾ç½®æœ€ä¼˜é…ç½®
"""

import os
import sys
import json
import platform
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional

def detect_system_info() -> Dict:
    """æ£€æµ‹ç³»ç»Ÿä¿¡æ¯"""
    info = {
        "os": platform.system(),
        "os_version": platform.version(),
        "python_version": sys.version,
        "architecture": platform.machine(),
    }
    
    # æ£€æµ‹Linuxå‘è¡Œç‰ˆ
    if info["os"] == "Linux":
        try:
            with open("/etc/os-release", "r") as f:
                os_release = f.read()
            for line in os_release.split("\n"):
                if line.startswith("ID="):
                    info["linux_distro"] = line.split("=")[1].strip('"')
                    break
        except:
            info["linux_distro"] = "unknown"
    
    return info

def detect_gpu_info() -> Dict:
    """æ£€æµ‹GPUä¿¡æ¯"""
    gpu_info = {
        "cuda_available": False,
        "gpu_count": 0,
        "gpu_names": [],
        "gpu_memory": [],
        "cuda_version": None
    }
    
    try:
        import torch
        gpu_info["cuda_available"] = torch.cuda.is_available()
        
        if gpu_info["cuda_available"]:
            gpu_info["gpu_count"] = torch.cuda.device_count()
            gpu_info["cuda_version"] = torch.version.cuda
            
            for i in range(gpu_info["gpu_count"]):
                name = torch.cuda.get_device_name(i)
                memory = torch.cuda.get_device_properties(i).total_memory // (1024**3)
                gpu_info["gpu_names"].append(name)
                gpu_info["gpu_memory"].append(memory)
    except ImportError:
        print("âš ï¸  PyTorch not installed, cannot detect GPU info")
    
    # æ£€æµ‹MPSæ”¯æŒ (Apple Silicon)
    try:
        import torch
        gpu_info["mps_available"] = hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
    except:
        gpu_info["mps_available"] = False
    
    return gpu_info

def check_dependencies() -> Dict:
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    deps = {
        "torch": False,
        "transformers": False,
        "timm": False,
        "flash_attn": False,
        "xformers": False,
        "bitsandbytes": False
    }
    
    for pkg in deps:
        try:
            __import__(pkg)
            deps[pkg] = True
        except ImportError:
            deps[pkg] = False
    
    return deps

def recommend_config(system_info: Dict, gpu_info: Dict) -> Dict:
    """æ ¹æ®ç¡¬ä»¶æ¨èé…ç½®"""
    config = {
        "device": "cpu",
        "vlm_enabled": False,
        "batch_size": 4,
        "fp16": False,
        "performance_priority": "balanced",
        "table_model_type": "slanet_plus",
        "memory_optimization": True,
        "reasoning": []
    }
    
    # è®¾å¤‡é€‰æ‹©
    if gpu_info["cuda_available"] and gpu_info["gpu_count"] > 0:
        config["device"] = "cuda:0"
        config["vlm_enabled"] = True
        config["fp16"] = True
        config["reasoning"].append("æ£€æµ‹åˆ°CUDA GPUï¼Œå¯ç”¨GPUåŠ é€Ÿå’ŒVLM")
        
        # æ ¹æ®GPUæ€§èƒ½è°ƒæ•´é…ç½®
        gpu_name = gpu_info["gpu_names"][0].lower() if gpu_info["gpu_names"] else ""
        gpu_memory = gpu_info["gpu_memory"][0] if gpu_info["gpu_memory"] else 0
        
        # é«˜ç«¯GPUé…ç½®
        if any(gpu in gpu_name for gpu in ["a100", "v100", "h100", "a6000"]):
            config.update({
                "batch_size": 32,
                "performance_priority": "accuracy",
                "table_model_type": "unitable",
                "memory_optimization": False,
                "min_batch_inference_size": 1024
            })
            config["reasoning"].append(f"æ£€æµ‹åˆ°é«˜ç«¯GPU ({gpu_name})ï¼Œä½¿ç”¨é«˜æ€§èƒ½é…ç½®")
            
        # ä¸­ç«¯GPUé…ç½®
        elif any(gpu in gpu_name for gpu in ["rtx", "gtx", "quadro"]) and gpu_memory >= 8:
            config.update({
                "batch_size": 16,
                "performance_priority": "balanced",
                "table_model_type": "unitable",
                "min_batch_inference_size": 512
            })
            config["reasoning"].append(f"æ£€æµ‹åˆ°ä¸­ç«¯GPU ({gpu_name}, {gpu_memory}GB)ï¼Œä½¿ç”¨å‡è¡¡é…ç½®")
            
        # ä½ç«¯GPUé…ç½®
        elif gpu_memory < 8:
            config.update({
                "batch_size": 8,
                "performance_priority": "speed", 
                "table_model_type": "slanet_plus",
                "memory_optimization": True,
                "min_batch_inference_size": 256
            })
            config["reasoning"].append(f"æ£€æµ‹åˆ°ä½æ˜¾å­˜GPU ({gpu_memory}GB)ï¼Œä½¿ç”¨å†…å­˜ä¼˜åŒ–é…ç½®")
            
    elif gpu_info["mps_available"]:
        config.update({
            "device": "mps",
            "vlm_enabled": True,
            "batch_size": 8,
            "fp16": True,
            "performance_priority": "balanced",
            "table_model_type": "auto"
        })
        config["reasoning"].append("æ£€æµ‹åˆ°Apple Silicon MPSï¼Œå¯ç”¨MPSåŠ é€Ÿ")
        
    else:
        config.update({
            "device": "cpu",
            "vlm_enabled": False,
            "batch_size": 4,
            "fp16": False,
            "performance_priority": "speed",
            "table_model_type": "slanet_plus"
        })
        config["reasoning"].append("ä»…æ£€æµ‹åˆ°CPUï¼Œä½¿ç”¨CPUä¼˜åŒ–é…ç½®")
    
    return config

def generate_env_file(config: Dict, output_path: str = ".mineru_env"):
    """ç”Ÿæˆç¯å¢ƒå˜é‡æ–‡ä»¶"""
    env_content = f"""# MinerU VLM Environment Configuration
# Auto-generated by setup_vlm_env.py

# Device Configuration
export MINERU_DEVICE_MODE={config['device']}
export MINERU_VLM_ENABLED={str(config['vlm_enabled']).lower()}

# Performance Configuration
export MINERU_MIN_BATCH_INFERENCE_SIZE={config.get('min_batch_inference_size', 384)}
export MINERU_PERFORMANCE_PRIORITY={config['performance_priority']}
export MINERU_TABLE_MODEL_TYPE={config['table_model_type']}

# Memory Configuration
export MINERU_MEMORY_OPTIMIZATION={str(config['memory_optimization']).lower()}
"""
    
    if config["device"].startswith("cuda"):
        env_content += f"""
# CUDA Optimization
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export TORCH_CUDNN_V8_API_ENABLED=1
export CUDNN_BENCHMARK=1
"""
        
        if config.get("min_batch_inference_size", 0) >= 1024:
            env_content += """
# High-end GPU Optimization
export MINERU_FLASH_ATTENTION=true
export MINERU_USE_XFORMERS=true
"""

    env_content += """
# Cache Configuration
export TRANSFORMERS_CACHE=${HOME}/.cache/transformers
export HF_HOME=${HOME}/.cache/huggingface

# Logging
export MINERU_LOG_LEVEL=INFO
"""
    
    with open(output_path, "w") as f:
        f.write(env_content)
    
    return output_path

def install_missing_dependencies(deps: Dict, gpu_info: Dict):
    """å®‰è£…ç¼ºå¤±çš„ä¾èµ–"""
    missing = [pkg for pkg, installed in deps.items() if not installed]
    
    if not missing:
        print("âœ… æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
        return
    
    print(f"ğŸ“¦ å‡†å¤‡å®‰è£…ç¼ºå¤±çš„ä¾èµ–: {', '.join(missing)}")
    
    install_commands = []
    
    # PyTorch
    if "torch" in missing:
        if gpu_info["cuda_available"]:
            install_commands.append("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        else:
            install_commands.append("pip install torch torchvision torchaudio")
    
    # Transformers
    if "transformers" in missing:
        install_commands.append("pip install transformers>=4.35.0 accelerate>=0.20.0 sentencepiece protobuf")
    
    # TIMM
    if "timm" in missing:
        install_commands.append("pip install timm>=0.9.0")
    
    # å¯é€‰ä¼˜åŒ–ä¾èµ–
    if gpu_info["cuda_available"]:
        optional_deps = []
        if "flash_attn" in missing:
            optional_deps.append("flash-attn")
        if "xformers" in missing:
            optional_deps.append("xformers")
        if "bitsandbytes" in missing:
            optional_deps.append("bitsandbytes")
        
        if optional_deps:
            install_commands.append(f"pip install {' '.join(optional_deps)}")
    
    # æ‰§è¡Œå®‰è£…
    for cmd in install_commands:
        print(f"æ‰§è¡Œ: {cmd}")
        try:
            subprocess.run(cmd.split(), check=True)
        except subprocess.CalledProcessError as e:
            print(f"âš ï¸  å®‰è£…å¤±è´¥: {e}")

def create_test_script(config: Dict):
    """åˆ›å»ºæµ‹è¯•è„šæœ¬"""
    test_content = f'''#!/usr/bin/env python3
"""
MinerU VLM é…ç½®æµ‹è¯•è„šæœ¬
"""

import sys
import time
from pathlib import Path

def test_imports():
    """æµ‹è¯•å¯¼å…¥"""
    try:
        import torch
        print(f"âœ… PyTorch {torch.__version__}")
        
        import transformers
        print(f"âœ… Transformers {transformers.__version__}")
        
        import timm
        print(f"âœ… TIMM {timm.__version__}")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    return True

def test_device():
    """æµ‹è¯•è®¾å¤‡"""
    import torch
    
    device = "{config['device']}"
    print(f"ç›®æ ‡è®¾å¤‡: {device}")
    
    if device.startswith("cuda"):
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"âœ… æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
        else:
            print("âŒ CUDA ä¸å¯ç”¨")
            return False
    elif device == "mps":
        if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            print("âœ… MPS å¯ç”¨")
        else:
            print("âŒ MPS ä¸å¯ç”¨") 
            return False
    else:
        print("âœ… CPU æ¨¡å¼")
    
    return True

def test_vlm():
    """æµ‹è¯•VLM"""
    if not {str(config['vlm_enabled']).lower()}:
        print("â„¹ï¸  VLM æœªå¯ç”¨")
        return True
        
    try:
        from mineru.backend.pipeline.model_init import VLM_AVAILABLE, TableVLMProcessor
        
        if not VLM_AVAILABLE:
            print("âŒ VLM ä¾èµ–ä¸å¯ç”¨")
            return False
            
        print("âœ… VLM ä¾èµ–å¯ç”¨")
        
        # æµ‹è¯•VLMå¤„ç†å™¨åˆ›å»º
        start = time.time()
        processor = TableVLMProcessor(device="{config['device']}")
        load_time = time.time() - start
        print(f"âœ… VLM å¤„ç†å™¨åˆ›å»ºæˆåŠŸ ({load_time:.1f}s)")
        
    except Exception as e:
        print(f"âŒ VLM æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    return True

def main():
    print("ğŸ” MinerU VLM é…ç½®æµ‹è¯•")
    print("=" * 40)
    
    # æµ‹è¯•å¯¼å…¥
    if not test_imports():
        sys.exit(1)
    
    # æµ‹è¯•è®¾å¤‡
    if not test_device():
        sys.exit(1)
    
    # æµ‹è¯•VLM
    if not test_vlm():
        sys.exit(1)
    
    print("=" * 40)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é…ç½®æ­£ç¡®ã€‚")
    print()
    print("ä½¿ç”¨ç¤ºä¾‹:")
    print("  source .mineru_env")
    print("  mineru -p document.pdf -o output/ {'--vlm' if config['vlm_enabled'] else ''}")

if __name__ == "__main__":
    main()
'''
    
    with open("test_vlm_config.py", "w") as f:
        f.write(test_content)
    
    os.chmod("test_vlm_config.py", 0o755)
    return "test_vlm_config.py"

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ MinerU VLM ç¯å¢ƒè®¾ç½®")
    print("=" * 50)
    
    # æ£€æµ‹ç³»ç»Ÿä¿¡æ¯
    print("ğŸ“‹ æ£€æµ‹ç³»ç»Ÿä¿¡æ¯...")
    system_info = detect_system_info()
    print(f"  æ“ä½œç³»ç»Ÿ: {system_info['os']} ({system_info.get('linux_distro', '')})")
    print(f"  æ¶æ„: {system_info['architecture']}")
    print(f"  Python: {sys.version.split()[0]}")
    
    # æ£€æµ‹GPUä¿¡æ¯
    print("\nğŸ” æ£€æµ‹GPUä¿¡æ¯...")
    gpu_info = detect_gpu_info()
    if gpu_info["cuda_available"]:
        for i, (name, memory) in enumerate(zip(gpu_info["gpu_names"], gpu_info["gpu_memory"])):
            print(f"  GPU {i}: {name} ({memory}GB)")
        print(f"  CUDAç‰ˆæœ¬: {gpu_info['cuda_version']}")
    elif gpu_info["mps_available"]:
        print("  æ£€æµ‹åˆ°Apple Silicon MPSæ”¯æŒ")
    else:
        print("  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
    
    # æ£€æŸ¥ä¾èµ–
    print("\nğŸ“¦ æ£€æŸ¥ä¾èµ–...")
    deps = check_dependencies()
    for pkg, installed in deps.items():
        status = "âœ…" if installed else "âŒ"
        print(f"  {pkg}: {status}")
    
    # æ¨èé…ç½®
    print("\nğŸ¯ ç”Ÿæˆæ¨èé…ç½®...")
    config = recommend_config(system_info, gpu_info)
    
    print("\næ¨èé…ç½®:")
    print(f"  è®¾å¤‡: {config['device']}")
    print(f"  VLMå¯ç”¨: {config['vlm_enabled']}")
    print(f"  æ‰¹å¤„ç†å¤§å°: {config['batch_size']}")
    print(f"  æ€§èƒ½ä¼˜å…ˆçº§: {config['performance_priority']}")
    print(f"  è¡¨æ ¼æ¨¡å‹: {config['table_model_type']}")
    
    print("\né…ç½®ç†ç”±:")
    for reason in config["reasoning"]:
        print(f"  â€¢ {reason}")
    
    # è¯¢é—®æ˜¯å¦å®‰è£…ä¾èµ–
    if not all(deps.values()):
        response = input("\næ˜¯å¦å®‰è£…ç¼ºå¤±çš„ä¾èµ–ï¼Ÿ (y/n): ")
        if response.lower() == 'y':
            install_missing_dependencies(deps, gpu_info)
    
    # ç”Ÿæˆç¯å¢ƒæ–‡ä»¶
    print("\nğŸ“ ç”Ÿæˆé…ç½®æ–‡ä»¶...")
    env_file = generate_env_file(config)
    print(f"  ç¯å¢ƒæ–‡ä»¶: {env_file}")
    
    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    test_script = create_test_script(config)
    print(f"  æµ‹è¯•è„šæœ¬: {test_script}")
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config_info = {
        "system_info": system_info,
        "gpu_info": gpu_info,
        "dependencies": deps,
        "recommended_config": config,
        "timestamp": time.time()
    }
    
    with open("mineru_config_info.json", "w") as f:
        json.dump(config_info, f, indent=2, default=str)
    
    print(f"  é…ç½®ä¿¡æ¯: mineru_config_info.json")
    
    print("\nğŸ‰ ç¯å¢ƒè®¾ç½®å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. åŠ è½½ç¯å¢ƒå˜é‡: source .mineru_env") 
    print("2. è¿è¡Œæµ‹è¯•: python test_vlm_config.py")
    print("3. å¼€å§‹ä½¿ç”¨: mineru -p document.pdf -o output/" + (" --vlm" if config['vlm_enabled'] else ""))

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)