#!/usr/bin/env python3
"""
M1 Max MacBook ä¼˜åŒ–çš„ Pipeline + VLM-Transformers æ··åˆè¡¨æ ¼å¤„ç†ç³»ç»Ÿ
ç»“åˆ MinerU Pipeline çš„ä¸“ä¸šæ€§å’Œ VLM çš„è¯­ä¹‰ç†è§£èƒ½åŠ›
"""

import os
import sys
import time
import torch
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from PIL import Image
import logging

# æ£€æŸ¥ M1 ä¼˜åŒ–ç¯å¢ƒ
def check_m1_environment():
    """æ£€æŸ¥ M1 Mac è¿è¡Œç¯å¢ƒ"""
    info = {}
    
    # æ£€æŸ¥ PyTorch MPS æ”¯æŒ
    info['pytorch_version'] = torch.__version__
    info['mps_available'] = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False
    info['mps_built'] = torch.backends.mps.is_built() if hasattr(torch.backends, 'mps') else False
    
    # æ£€æŸ¥è®¾å¤‡
    if info['mps_available']:
        info['device'] = 'mps'
    elif torch.cuda.is_available():
        info['device'] = 'cuda'
    else:
        info['device'] = 'cpu'
    
    # ç³»ç»Ÿä¿¡æ¯
    import platform
    info['platform'] = platform.platform()
    info['processor'] = platform.processor()
    info['is_m1_mac'] = 'arm64' in platform.platform().lower()
    
    return info

@dataclass 
class M1TableProcessingConfig:
    """M1 ä¼˜åŒ–é…ç½®"""
    # Pipeline é…ç½®
    pipeline_device: str = 'mps'
    pipeline_batch_size: int = 4
    pipeline_precision: str = 'fp16'  # M1 å¯¹ fp16 ä¼˜åŒ–å¾ˆå¥½
    
    # VLM é…ç½®
    vlm_model_name: str = 'microsoft/table-transformer-structure-recognition'  # å¤‡é€‰æ–¹æ¡ˆ
    vlm_device: str = 'mps'
    vlm_batch_size: int = 1
    vlm_max_length: int = 512
    vlm_use_flash_attention: bool = False  # M1 æš‚ä¸æ”¯æŒ
    
    # æ··åˆç­–ç•¥é…ç½®
    complexity_threshold: float = 0.4
    semantic_threshold: float = 0.3
    quality_threshold: float = 0.6
    
    # M1 æ€§èƒ½ä¼˜åŒ–
    use_mlx: bool = False  # æ˜¯å¦ä½¿ç”¨ MLX æ¡†æ¶
    memory_optimization: bool = True
    concurrent_processing: bool = True

class M1OptimizedVLMProcessor:
    """M1 ä¼˜åŒ–çš„ VLM å¤„ç†å™¨"""
    
    def __init__(self, config: M1TableProcessingConfig):
        self.config = config
        self.device = config.vlm_device
        self.logger = logging.getLogger(__name__)
        
        # ç¯å¢ƒæ£€æŸ¥
        self.env_info = check_m1_environment()
        self.logger.info(f"M1 Environment: {self.env_info}")
        
        # åˆå§‹åŒ– VLM æ¨¡å‹
        self._init_vlm_models()
        
    def _init_vlm_models(self):
        """åˆå§‹åŒ–é€‚åˆ M1 çš„ VLM æ¨¡å‹"""
        try:
            # å°è¯•ä½¿ç”¨ MLX ä¼˜åŒ–æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.use_mlx:
                self._init_mlx_models()
            else:
                self._init_huggingface_models()
                
        except Exception as e:
            self.logger.error(f"VLM model initialization failed: {e}")
            # é™çº§åˆ°åŸºç¡€æ¨¡å‹
            self._init_fallback_models()
    
    def _init_huggingface_models(self):
        """åˆå§‹åŒ– Hugging Face VLM æ¨¡å‹"""
        from transformers import AutoProcessor, AutoModelForCausalLM
        
        # æ¨èçš„è½»é‡ VLM æ¨¡å‹ï¼ˆé€‚åˆ M1ï¼‰
        model_options = [
            "microsoft/table-transformer-structure-recognition",  # ä¸“é—¨çš„è¡¨æ ¼æ¨¡å‹
            "Salesforce/blip2-opt-2.7b",  # è½»é‡é€šç”¨ VLM  
            "llava-hf/llava-1.5-7b-hf",   # LLaVA æ¨¡å‹
        ]
        
        model_name = self.config.vlm_model_name
        if model_name not in model_options:
            model_name = model_options[0]  # é»˜è®¤ä½¿ç”¨è¡¨æ ¼ä¸“ç”¨æ¨¡å‹
            
        self.logger.info(f"Loading VLM model: {model_name}")
        
        # é…ç½®æ¨¡å‹åŠ è½½å‚æ•°ï¼ˆM1 ä¼˜åŒ–ï¼‰
        model_kwargs = {
            'torch_dtype': torch.float16,  # M1 å¯¹ fp16 å‹å¥½
            'device_map': 'auto' if self.device == 'mps' else None,
            'trust_remote_code': True
        }
        
        if self.config.memory_optimization:
            model_kwargs.update({
                'low_cpu_mem_usage': True,
                'load_in_8bit': False,  # M1 æš‚ä¸å®Œå…¨æ”¯æŒé‡åŒ–
                'load_in_4bit': False
            })
        
        # åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device == 'mps':
            self.model = self.model.to('mps')
            
        self.model.eval()
        self.logger.info(f"VLM model loaded successfully on {self.device}")
        
    def _init_mlx_models(self):
        """åˆå§‹åŒ– MLX ä¼˜åŒ–æ¨¡å‹ï¼ˆApple Silicon ä¸“ç”¨ï¼‰"""
        try:
            # å°è¯•å¯¼å…¥ MLX VLM
            import mlx.core as mx
            from mlx_vlm import load, generate
            
            # MLX ä¼˜åŒ–çš„æ¨¡å‹åˆ—è¡¨
            mlx_models = [
                "mlx-community/llava-1.5-7b-mlx",
                "mlx-community/blip2-mlx", 
            ]
            
            model_name = mlx_models[0]  # é»˜è®¤ä½¿ç”¨ LLaVA MLX ç‰ˆæœ¬
            self.logger.info(f"Loading MLX optimized model: {model_name}")
            
            self.mlx_model, self.mlx_processor = load(model_name)
            self.use_mlx = True
            
            self.logger.info("MLX VLM model loaded successfully")
            
        except ImportError:
            self.logger.warning("MLX not available, falling back to PyTorch")
            self._init_huggingface_models()
        except Exception as e:
            self.logger.error(f"MLX model loading failed: {e}")
            self._init_huggingface_models()
    
    def _init_fallback_models(self):
        """åˆå§‹åŒ–å¤‡é€‰åŸºç¡€æ¨¡å‹"""
        self.logger.info("Initializing fallback table processing model")
        
        # ä½¿ç”¨æ›´è½»é‡çš„æ›¿ä»£æ–¹æ¡ˆ
        self.use_simple_vlm = True
        self.logger.warning("Using simplified VLM processing")
        
    def process_table_with_vlm(
        self, 
        image: Image.Image, 
        context: Optional[str] = None,
        task_type: str = "structure_recognition"
    ) -> Dict[str, Any]:
        """ä½¿ç”¨ VLM å¤„ç†è¡¨æ ¼"""
        start_time = time.time()
        
        try:
            if hasattr(self, 'use_mlx') and self.use_mlx:
                return self._process_with_mlx(image, context, task_type)
            elif hasattr(self, 'use_simple_vlm') and self.use_simple_vlm:
                return self._process_with_simple_vlm(image, context)
            else:
                return self._process_with_huggingface(image, context, task_type)
                
        except Exception as e:
            self.logger.error(f"VLM processing failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def _process_with_huggingface(self, image: Image.Image, context: str, task_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Hugging Face æ¨¡å‹å¤„ç†"""
        
        # æ„å»ºæç¤º
        if task_type == "structure_recognition":
            prompt = "Analyze the table structure in this image and describe the layout, cells, and organization."
        elif task_type == "semantic_understanding":
            prompt = f"Understand this table's content and meaning. Context: {context or 'None'}"
        else:
            prompt = "Describe what you see in this table image."
        
        # é¢„å¤„ç†
        inputs = self.processor(
            text=prompt,
            images=image,
            return_tensors="pt",
            padding=True
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        if self.device == 'mps':
            inputs = {k: v.to('mps') if hasattr(v, 'to') else v for k, v in inputs.items()}
        
        # æ¨ç†
        with torch.no_grad():
            # M1 ä¼˜åŒ–æ¨ç†å‚æ•°
            generate_kwargs = {
                'max_length': self.config.vlm_max_length,
                'do_sample': True,
                'temperature': 0.7,
                'top_p': 0.9,
                'pad_token_id': self.processor.tokenizer.eos_token_id
            }
            
            outputs = self.model.generate(**inputs, **generate_kwargs)
        
        # è§£ç ç»“æœ
        response = self.processor.decode(outputs[0], skip_special_tokens=True)
        
        return {
            'success': True,
            'response': response,
            'confidence': 0.85,  # ç®€åŒ–çš„ç½®ä¿¡åº¦
            'task_type': task_type,
            'model_type': 'huggingface'
        }
    
    def _process_with_mlx(self, image: Image.Image, context: str, task_type: str) -> Dict[str, Any]:
        """ä½¿ç”¨ MLX ä¼˜åŒ–æ¨¡å‹å¤„ç†"""
        from mlx_vlm import generate
        
        # æ„å»ºæç¤º
        prompt = f"Analyze this table image. Task: {task_type}. Context: {context or 'None'}"
        
        # MLX æ¨ç†
        response = generate(
            self.mlx_model,
            self.mlx_processor, 
            image,
            prompt,
            max_tokens=self.config.vlm_max_length,
            temp=0.7
        )
        
        return {
            'success': True,
            'response': response,
            'confidence': 0.90,  # MLX ä¼˜åŒ–é€šå¸¸æ›´å‡†ç¡®
            'task_type': task_type,
            'model_type': 'mlx'
        }
    
    def _process_with_simple_vlm(self, image: Image.Image, context: str) -> Dict[str, Any]:
        """ç®€åŒ–çš„ VLM å¤„ç†ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰"""
        
        # åŸºç¡€å›¾åƒåˆ†æ
        width, height = image.size
        aspect_ratio = height / width if width > 0 else 1.0
        
        # ç®€åŒ–çš„è¡¨æ ¼åˆ†æ
        analysis = {
            'structure': 'standard_table' if 0.5 < aspect_ratio < 2.0 else 'complex_layout',
            'estimated_cells': min(max(int((width * height) / 10000), 4), 100),
            'image_quality': 'good' if width > 800 and height > 600 else 'low'
        }
        
        response = f"Table analysis: {analysis['structure']}, estimated {analysis['estimated_cells']} cells, {analysis['image_quality']} quality"
        
        return {
            'success': True,
            'response': response,
            'confidence': 0.60,  # è¾ƒä½ç½®ä¿¡åº¦
            'task_type': 'simple_analysis',
            'model_type': 'fallback',
            'analysis': analysis
        }

class M1PipelineVLMHybrid:
    """M1 ä¼˜åŒ–çš„ Pipeline + VLM æ··åˆå¤„ç†å™¨"""
    
    def __init__(self, config: M1TableProcessingConfig = None):
        self.config = config or M1TableProcessingConfig()
        self.logger = logging.getLogger(__name__)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'pipeline_only': {'count': 0, 'total_time': 0, 'accuracy': []},
            'vlm_only': {'count': 0, 'total_time': 0, 'accuracy': []},
            'hybrid': {'count': 0, 'total_time': 0, 'accuracy': []}
        }
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._init_pipeline()
        self._init_vlm_processor()
        
    def _init_pipeline(self):
        """åˆå§‹åŒ– MinerU Pipelineï¼ˆM1 ä¼˜åŒ–ï¼‰"""
        try:
            # å¯¼å…¥ MinerU ç»„ä»¶
            sys.path.append('/Users/frank/mygit/github/MinerU')
            from mineru.backend.pipeline.model_init import MineruPipelineModel
            
            pipeline_config = {
                'table_config': {
                    'enable': True,
                    'model_type': 'auto',  # è‡ªåŠ¨é€‰æ‹©é€‚åˆ M1 çš„æ¨¡å‹
                    'model_selection': 'auto'
                },
                'formula_config': {'enable': True},
                'device': self.config.pipeline_device,
                'performance_priority': 'balanced'
            }
            
            self.pipeline = MineruPipelineModel(**pipeline_config)
            self.logger.info("MinerU Pipeline initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Pipeline initialization failed: {e}")
            self.pipeline = None
    
    def _init_vlm_processor(self):
        """åˆå§‹åŒ– VLM å¤„ç†å™¨"""
        self.vlm_processor = M1OptimizedVLMProcessor(self.config)
        
    def analyze_table_complexity(self, image: Image.Image) -> Dict[str, float]:
        """åˆ†æè¡¨æ ¼å¤æ‚åº¦ï¼ˆé’ˆå¯¹ M1 ä¼˜åŒ–çš„è½»é‡ç‰ˆæœ¬ï¼‰"""
        import cv2
        
        # è½¬æ¢ä¸º numpy æ•°ç»„
        img_array = np.array(image)
        if len(img_array.shape) == 3:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = img_array
            
        height, width = gray.shape
        
        # å¿«é€Ÿå¤æ‚åº¦åˆ†æï¼ˆM1 ä¼˜åŒ–ï¼‰
        try:
            # 1. è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / (height * width)
            
            # 2. çº¿æ¡æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=50, 
                                  minLineLength=min(width, height)//4, maxLineGap=10)
            line_count = len(lines) if lines is not None else 0
            
            # 3. æ–‡æœ¬å¯†åº¦ä¼°ç®—
            binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
            text_density = np.sum(binary == 0) / (height * width)
            
            # 4. å›¾åƒæ–¹å·®
            image_variance = np.var(gray) / (255 * 255)
            
            # ç»¼åˆè¯„åˆ†
            visual_complexity = min(edge_density * 2 + image_variance, 1.0)
            structural_complexity = min(line_count / 20 + text_density, 1.0)
            
            return {
                'visual_complexity': visual_complexity,
                'structural_complexity': structural_complexity,
                'overall_complexity': (visual_complexity + structural_complexity) / 2,
                'line_count': line_count,
                'edge_density': edge_density,
                'text_density': text_density
            }
            
        except Exception as e:
            self.logger.warning(f"Complexity analysis failed: {e}")
            return {
                'visual_complexity': 0.5,
                'structural_complexity': 0.5,
                'overall_complexity': 0.5,
                'error': str(e)
            }
    
    def select_processing_strategy(self, 
                                 image: Image.Image, 
                                 context: Optional[str] = None,
                                 user_priority: str = 'balanced') -> str:
        """é€‰æ‹©å¤„ç†ç­–ç•¥"""
        
        # åˆ†æå¤æ‚åº¦
        complexity = self.analyze_table_complexity(image)
        overall_complexity = complexity['overall_complexity']
        
        # åˆ†æè¯­ä¹‰éœ€æ±‚
        semantic_score = 0.2  # åŸºç¡€è¯­ä¹‰éœ€æ±‚
        if context:
            semantic_keywords = ['financial', 'analysis', 'comparison', 'calculation']
            for keyword in semantic_keywords:
                if keyword in context.lower():
                    semantic_score = 0.8
                    break
        
        # å›¾åƒè´¨é‡è¯„ä¼°ï¼ˆç®€åŒ–ï¼‰
        width, height = image.size
        image_quality = min((width * height) / (1920 * 1080), 1.0)
        
        # å†³ç­–é€»è¾‘
        if user_priority == 'speed':
            if overall_complexity < self.config.complexity_threshold:
                return 'pipeline_only'
            else:
                return 'pipeline_only'  # é€Ÿåº¦ä¼˜å…ˆå§‹ç»ˆç”¨ Pipeline
                
        elif user_priority == 'accuracy':
            if semantic_score > self.config.semantic_threshold:
                return 'vlm_only'
            elif overall_complexity > self.config.complexity_threshold:
                return 'hybrid'
            else:
                return 'pipeline_only'
                
        else:  # balanced
            if overall_complexity < 0.3:
                return 'pipeline_only'
            elif semantic_score > 0.5 or overall_complexity > 0.7:
                return 'hybrid'
            else:
                return 'pipeline_only'
    
    def process_table(self, 
                     image: Image.Image,
                     context: Optional[str] = None,
                     user_priority: str = 'balanced') -> Dict[str, Any]:
        """æ··åˆè¡¨æ ¼å¤„ç†ä¸»æ¥å£"""
        
        start_time = time.time()
        
        # é€‰æ‹©å¤„ç†ç­–ç•¥
        strategy = self.select_processing_strategy(image, context, user_priority)
        
        self.logger.info(f"Selected processing strategy: {strategy}")
        
        # æ‰§è¡Œå¤„ç†
        if strategy == 'pipeline_only':
            result = self._process_pipeline_only(image)
            self.performance_stats['pipeline_only']['count'] += 1
        elif strategy == 'vlm_only':
            result = self._process_vlm_only(image, context)
            self.performance_stats['vlm_only']['count'] += 1
        else:  # hybrid
            result = self._process_hybrid(image, context)
            self.performance_stats['hybrid']['count'] += 1
        
        # è®°å½•æ€§èƒ½
        processing_time = time.time() - start_time
        result['processing_time'] = processing_time
        result['strategy'] = strategy
        
        # æ›´æ–°ç»Ÿè®¡
        stats = self.performance_stats[strategy.replace('_only', '')]
        stats['total_time'] += processing_time
        
        self.logger.info(f"Processing completed in {processing_time:.2f}s using {strategy}")
        
        return result
    
    def _process_pipeline_only(self, image: Image.Image) -> Dict[str, Any]:
        """çº¯ Pipeline å¤„ç†"""
        if not self.pipeline:
            return {'success': False, 'error': 'Pipeline not available'}
        
        try:
            # ä½¿ç”¨ MinerU Pipeline å¤„ç†
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™… MinerU API è°ƒæ•´
            if hasattr(self.pipeline, 'table_model'):
                result = self.pipeline.table_model.predict(image)
                
                return {
                    'success': True,
                    'html': result[0] if result[0] else '',
                    'cell_bboxes': result[1] if len(result) > 1 else [],
                    'confidence': 0.92,
                    'method': 'pipeline_only'
                }
            else:
                return {'success': False, 'error': 'Table model not available'}
                
        except Exception as e:
            self.logger.error(f"Pipeline processing failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _process_vlm_only(self, image: Image.Image, context: str) -> Dict[str, Any]:
        """çº¯ VLM å¤„ç†"""
        vlm_result = self.vlm_processor.process_table_with_vlm(
            image, context, "structure_recognition"
        )
        
        if vlm_result['success']:
            return {
                'success': True,
                'html': self._convert_vlm_to_html(vlm_result['response']),
                'semantic_analysis': vlm_result['response'],
                'confidence': vlm_result['confidence'],
                'method': 'vlm_only',
                'model_type': vlm_result.get('model_type', 'unknown')
            }
        else:
            return vlm_result
    
    def _process_hybrid(self, image: Image.Image, context: str) -> Dict[str, Any]:
        """æ··åˆå¤„ç†"""
        
        if self.config.concurrent_processing:
            # å¹¶è¡Œå¤„ç†
            import concurrent.futures
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                pipeline_future = executor.submit(self._process_pipeline_only, image)
                vlm_future = executor.submit(self._process_vlm_only, image, context)
                
                pipeline_result = pipeline_future.result()
                vlm_result = vlm_future.result()
        else:
            # ä¸²è¡Œå¤„ç†
            pipeline_result = self._process_pipeline_only(image)
            if pipeline_result['success'] and pipeline_result.get('confidence', 0) > 0.9:
                # Pipeline ç»“æœå¾ˆå¥½ï¼Œæ— éœ€ VLM éªŒè¯
                return {**pipeline_result, 'method': 'hybrid_pipeline_sufficient'}
            
            vlm_result = self._process_vlm_only(image, context)
        
        # èåˆç»“æœ
        return self._fuse_results(pipeline_result, vlm_result)
    
    def _fuse_results(self, pipeline_result: Dict, vlm_result: Dict) -> Dict[str, Any]:
        """èåˆ Pipeline å’Œ VLM ç»“æœ"""
        
        if not pipeline_result['success'] and not vlm_result['success']:
            return {'success': False, 'error': 'Both methods failed'}
        
        if not pipeline_result['success']:
            return {**vlm_result, 'method': 'hybrid_vlm_fallback'}
        
        if not vlm_result['success']:
            return {**pipeline_result, 'method': 'hybrid_pipeline_fallback'}
        
        # ä¸¤ä¸ªéƒ½æˆåŠŸï¼Œé€‰æ‹©æ›´å¯ä¿¡çš„ç»“æœ
        pipeline_conf = pipeline_result.get('confidence', 0.5)
        vlm_conf = vlm_result.get('confidence', 0.5)
        
        if vlm_conf > pipeline_conf + 0.1:
            # VLM æ˜æ˜¾æ›´å¥½
            return {
                **vlm_result,
                'method': 'hybrid_vlm_primary',
                'pipeline_backup': pipeline_result
            }
        else:
            # Pipeline ä¸ºä¸»ï¼ŒVLM å¢å¼º
            return {
                **pipeline_result,
                'method': 'hybrid_pipeline_primary',
                'semantic_enhancement': vlm_result.get('semantic_analysis'),
                'vlm_confidence': vlm_conf
            }
    
    def _convert_vlm_to_html(self, vlm_response: str) -> str:
        """å°† VLM å“åº”è½¬æ¢ä¸º HTMLï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è½¬æ¢ï¼Œå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„ NLP å¤„ç†
        if 'table' in vlm_response.lower():
            return "<table><tr><td>VLM Generated Content</td></tr></table>"
        return ""
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡æ‘˜è¦"""
        summary = {}
        
        for strategy, stats in self.performance_stats.items():
            if stats['count'] > 0:
                avg_time = stats['total_time'] / stats['count']
                summary[strategy] = {
                    'count': stats['count'],
                    'average_time': avg_time,
                    'total_time': stats['total_time']
                }
            else:
                summary[strategy] = {'count': 0, 'average_time': 0, 'total_time': 0}
        
        return summary

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
def demo_m1_hybrid_processing():
    """æ¼”ç¤º M1 æ··åˆå¤„ç†"""
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, 
                       format='%(asctime)s - %(levelname)s - %(message)s')
    
    # æ£€æŸ¥ç¯å¢ƒ
    env_info = check_m1_environment()
    print("ğŸ–¥ï¸  M1 Mac ç¯å¢ƒä¿¡æ¯:")
    for key, value in env_info.items():
        print(f"   {key}: {value}")
    print()
    
    # åˆ›å»ºé…ç½®
    config = M1TableProcessingConfig(
        pipeline_device='mps' if env_info['mps_available'] else 'cpu',
        vlm_device='mps' if env_info['mps_available'] else 'cpu',
        use_mlx=False,  # å¯ä»¥å°è¯•è®¾ç½®ä¸º True
        memory_optimization=True,
        concurrent_processing=True
    )
    
    # åˆ›å»ºæ··åˆå¤„ç†å™¨
    print("ğŸš€ åˆå§‹åŒ– M1 Pipeline + VLM æ··åˆå¤„ç†å™¨...")
    processor = M1PipelineVLMHybrid(config)
    
    # æ¨¡æ‹Ÿè¡¨æ ¼å›¾åƒ
    test_scenarios = [
        {
            'name': 'ç®€å•è´¢åŠ¡è¡¨æ ¼',
            'image_size': (800, 600),
            'context': 'quarterly revenue report',
            'priority': 'speed'
        },
        {
            'name': 'å¤æ‚ç§‘ç ”æ•°æ®è¡¨',
            'image_size': (1200, 900),
            'context': 'scientific research data with statistical analysis',
            'priority': 'accuracy'
        },
        {
            'name': 'å¹³è¡¡å¤„ç†åœºæ™¯',
            'image_size': (1000, 700),
            'context': 'business analysis report',
            'priority': 'balanced'
        }
    ]
    
    print("\\nğŸ“Š å¼€å§‹æµ‹è¯•ä¸åŒåœºæ™¯...")
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\\n--- åœºæ™¯ {i}: {scenario['name']} ---")
        
        # åˆ›å»ºæ¨¡æ‹Ÿå›¾åƒ
        width, height = scenario['image_size']
        mock_image = Image.new('RGB', (width, height), color='white')
        
        # å¤„ç†è¡¨æ ¼
        result = processor.process_table(
            image=mock_image,
            context=scenario['context'],
            user_priority=scenario['priority']
        )
        
        print(f"âœ… å¤„ç†ç­–ç•¥: {result['strategy']}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}s")
        print(f"ğŸ“ˆ å¤„ç†æˆåŠŸ: {result['success']}")
        
        if result['success']:
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}")
            print(f"ğŸ”§ æ–¹æ³•: {result.get('method', 'unknown')}")
    
    # æ€§èƒ½ç»Ÿè®¡
    print("\\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡æ‘˜è¦:")
    summary = processor.get_performance_summary()
    for strategy, stats in summary.items():
        if stats['count'] > 0:
            print(f"   {strategy}: {stats['count']}æ¬¡, å¹³å‡{stats['average_time']:.3f}s")
    
    print("\\nğŸ‰ M1 æ··åˆå¤„ç†æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    demo_m1_hybrid_processing()